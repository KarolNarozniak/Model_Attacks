{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 15.0,
  "eval_steps": 500,
  "global_step": 4260,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.176056338028169,
      "grad_norm": 1.8495711088180542,
      "learning_rate": 2.965492957746479e-05,
      "loss": 5.5357,
      "step": 50
    },
    {
      "epoch": 0.352112676056338,
      "grad_norm": 2.3784945011138916,
      "learning_rate": 2.930281690140845e-05,
      "loss": 5.2901,
      "step": 100
    },
    {
      "epoch": 0.528169014084507,
      "grad_norm": 2.0766940116882324,
      "learning_rate": 2.8950704225352114e-05,
      "loss": 5.0745,
      "step": 150
    },
    {
      "epoch": 0.704225352112676,
      "grad_norm": 2.0431008338928223,
      "learning_rate": 2.8598591549295777e-05,
      "loss": 4.8583,
      "step": 200
    },
    {
      "epoch": 0.8802816901408451,
      "grad_norm": 1.7968285083770752,
      "learning_rate": 2.8246478873239436e-05,
      "loss": 4.7199,
      "step": 250
    },
    {
      "epoch": 1.056338028169014,
      "grad_norm": 2.3120107650756836,
      "learning_rate": 2.78943661971831e-05,
      "loss": 4.6197,
      "step": 300
    },
    {
      "epoch": 1.232394366197183,
      "grad_norm": 2.1501083374023438,
      "learning_rate": 2.754225352112676e-05,
      "loss": 4.5082,
      "step": 350
    },
    {
      "epoch": 1.408450704225352,
      "grad_norm": 2.077430248260498,
      "learning_rate": 2.7190140845070425e-05,
      "loss": 4.4308,
      "step": 400
    },
    {
      "epoch": 1.584507042253521,
      "grad_norm": 1.6460736989974976,
      "learning_rate": 2.6838028169014084e-05,
      "loss": 4.3988,
      "step": 450
    },
    {
      "epoch": 1.76056338028169,
      "grad_norm": 2.4507758617401123,
      "learning_rate": 2.6485915492957747e-05,
      "loss": 4.3465,
      "step": 500
    },
    {
      "epoch": 1.936619718309859,
      "grad_norm": 2.153836488723755,
      "learning_rate": 2.6133802816901406e-05,
      "loss": 4.3296,
      "step": 550
    },
    {
      "epoch": 2.112676056338028,
      "grad_norm": 2.2613778114318848,
      "learning_rate": 2.5781690140845072e-05,
      "loss": 4.2879,
      "step": 600
    },
    {
      "epoch": 2.288732394366197,
      "grad_norm": 2.213721513748169,
      "learning_rate": 2.5429577464788735e-05,
      "loss": 4.2476,
      "step": 650
    },
    {
      "epoch": 2.464788732394366,
      "grad_norm": 2.436262607574463,
      "learning_rate": 2.5077464788732394e-05,
      "loss": 4.2333,
      "step": 700
    },
    {
      "epoch": 2.640845070422535,
      "grad_norm": 1.7261648178100586,
      "learning_rate": 2.4725352112676057e-05,
      "loss": 4.2229,
      "step": 750
    },
    {
      "epoch": 2.816901408450704,
      "grad_norm": 2.576016902923584,
      "learning_rate": 2.437323943661972e-05,
      "loss": 4.192,
      "step": 800
    },
    {
      "epoch": 2.992957746478873,
      "grad_norm": 2.3186399936676025,
      "learning_rate": 2.4021126760563383e-05,
      "loss": 4.159,
      "step": 850
    },
    {
      "epoch": 3.169014084507042,
      "grad_norm": 2.091782569885254,
      "learning_rate": 2.3669014084507042e-05,
      "loss": 4.0813,
      "step": 900
    },
    {
      "epoch": 3.345070422535211,
      "grad_norm": 1.630483627319336,
      "learning_rate": 2.3316901408450705e-05,
      "loss": 4.1249,
      "step": 950
    },
    {
      "epoch": 3.52112676056338,
      "grad_norm": 2.265349864959717,
      "learning_rate": 2.2964788732394368e-05,
      "loss": 4.1071,
      "step": 1000
    },
    {
      "epoch": 3.697183098591549,
      "grad_norm": 1.9297289848327637,
      "learning_rate": 2.261267605633803e-05,
      "loss": 4.0618,
      "step": 1050
    },
    {
      "epoch": 3.873239436619718,
      "grad_norm": 2.853388547897339,
      "learning_rate": 2.226056338028169e-05,
      "loss": 4.0408,
      "step": 1100
    },
    {
      "epoch": 4.049295774647887,
      "grad_norm": 2.6090445518493652,
      "learning_rate": 2.1908450704225353e-05,
      "loss": 4.0478,
      "step": 1150
    },
    {
      "epoch": 4.225352112676056,
      "grad_norm": 2.292222499847412,
      "learning_rate": 2.1556338028169015e-05,
      "loss": 4.0097,
      "step": 1200
    },
    {
      "epoch": 4.401408450704225,
      "grad_norm": 2.398895025253296,
      "learning_rate": 2.1204225352112678e-05,
      "loss": 3.9774,
      "step": 1250
    },
    {
      "epoch": 4.577464788732394,
      "grad_norm": 2.4611496925354004,
      "learning_rate": 2.0852112676056337e-05,
      "loss": 3.9454,
      "step": 1300
    },
    {
      "epoch": 4.753521126760563,
      "grad_norm": 2.3576858043670654,
      "learning_rate": 2.05e-05,
      "loss": 3.9638,
      "step": 1350
    },
    {
      "epoch": 4.929577464788732,
      "grad_norm": 2.0223326683044434,
      "learning_rate": 2.014788732394366e-05,
      "loss": 3.941,
      "step": 1400
    },
    {
      "epoch": 5.105633802816901,
      "grad_norm": 2.2912235260009766,
      "learning_rate": 1.9795774647887326e-05,
      "loss": 3.8905,
      "step": 1450
    },
    {
      "epoch": 5.28169014084507,
      "grad_norm": 1.7242507934570312,
      "learning_rate": 1.9443661971830985e-05,
      "loss": 3.9147,
      "step": 1500
    },
    {
      "epoch": 5.457746478873239,
      "grad_norm": 2.0234148502349854,
      "learning_rate": 1.9091549295774648e-05,
      "loss": 3.9026,
      "step": 1550
    },
    {
      "epoch": 5.633802816901408,
      "grad_norm": 2.504484176635742,
      "learning_rate": 1.8739436619718307e-05,
      "loss": 3.8719,
      "step": 1600
    },
    {
      "epoch": 5.809859154929578,
      "grad_norm": 2.31388521194458,
      "learning_rate": 1.8387323943661973e-05,
      "loss": 3.8227,
      "step": 1650
    },
    {
      "epoch": 5.985915492957746,
      "grad_norm": 1.8579622507095337,
      "learning_rate": 1.8035211267605636e-05,
      "loss": 3.8359,
      "step": 1700
    },
    {
      "epoch": 6.161971830985916,
      "grad_norm": 2.6336021423339844,
      "learning_rate": 1.7683098591549296e-05,
      "loss": 3.7985,
      "step": 1750
    },
    {
      "epoch": 6.338028169014084,
      "grad_norm": 1.9163938760757446,
      "learning_rate": 1.733098591549296e-05,
      "loss": 3.8207,
      "step": 1800
    },
    {
      "epoch": 6.514084507042254,
      "grad_norm": 2.599277973175049,
      "learning_rate": 1.697887323943662e-05,
      "loss": 3.7845,
      "step": 1850
    },
    {
      "epoch": 6.690140845070422,
      "grad_norm": 2.7765612602233887,
      "learning_rate": 1.6626760563380284e-05,
      "loss": 3.7705,
      "step": 1900
    },
    {
      "epoch": 6.866197183098592,
      "grad_norm": 2.316626787185669,
      "learning_rate": 1.6274647887323943e-05,
      "loss": 3.7288,
      "step": 1950
    },
    {
      "epoch": 7.042253521126761,
      "grad_norm": 2.7742393016815186,
      "learning_rate": 1.5922535211267606e-05,
      "loss": 3.7811,
      "step": 2000
    },
    {
      "epoch": 7.21830985915493,
      "grad_norm": 2.12400221824646,
      "learning_rate": 1.557042253521127e-05,
      "loss": 3.7224,
      "step": 2050
    },
    {
      "epoch": 7.394366197183099,
      "grad_norm": 2.0366930961608887,
      "learning_rate": 1.5218309859154932e-05,
      "loss": 3.7284,
      "step": 2100
    },
    {
      "epoch": 7.570422535211268,
      "grad_norm": 1.970112919807434,
      "learning_rate": 1.4866197183098593e-05,
      "loss": 3.7071,
      "step": 2150
    },
    {
      "epoch": 7.746478873239437,
      "grad_norm": 1.986663818359375,
      "learning_rate": 1.4514084507042254e-05,
      "loss": 3.7034,
      "step": 2200
    },
    {
      "epoch": 7.922535211267606,
      "grad_norm": 2.1591646671295166,
      "learning_rate": 1.4161971830985916e-05,
      "loss": 3.6694,
      "step": 2250
    },
    {
      "epoch": 8.098591549295774,
      "grad_norm": 2.0472543239593506,
      "learning_rate": 1.3809859154929578e-05,
      "loss": 3.6951,
      "step": 2300
    },
    {
      "epoch": 8.274647887323944,
      "grad_norm": 1.4169683456420898,
      "learning_rate": 1.345774647887324e-05,
      "loss": 3.6572,
      "step": 2350
    },
    {
      "epoch": 8.450704225352112,
      "grad_norm": 2.256277561187744,
      "learning_rate": 1.3105633802816901e-05,
      "loss": 3.6499,
      "step": 2400
    },
    {
      "epoch": 8.626760563380282,
      "grad_norm": 1.8261289596557617,
      "learning_rate": 1.2753521126760564e-05,
      "loss": 3.6508,
      "step": 2450
    },
    {
      "epoch": 8.80281690140845,
      "grad_norm": 2.0900778770446777,
      "learning_rate": 1.2401408450704225e-05,
      "loss": 3.628,
      "step": 2500
    },
    {
      "epoch": 8.97887323943662,
      "grad_norm": 2.110692024230957,
      "learning_rate": 1.2049295774647888e-05,
      "loss": 3.5895,
      "step": 2550
    },
    {
      "epoch": 9.154929577464788,
      "grad_norm": 1.8445415496826172,
      "learning_rate": 1.1697183098591549e-05,
      "loss": 3.6088,
      "step": 2600
    },
    {
      "epoch": 9.330985915492958,
      "grad_norm": 2.4973018169403076,
      "learning_rate": 1.1345070422535212e-05,
      "loss": 3.5956,
      "step": 2650
    },
    {
      "epoch": 9.507042253521126,
      "grad_norm": 2.737478256225586,
      "learning_rate": 1.0992957746478873e-05,
      "loss": 3.6097,
      "step": 2700
    },
    {
      "epoch": 9.683098591549296,
      "grad_norm": 2.480708599090576,
      "learning_rate": 1.0640845070422536e-05,
      "loss": 3.6135,
      "step": 2750
    },
    {
      "epoch": 9.859154929577464,
      "grad_norm": 2.636070966720581,
      "learning_rate": 1.0288732394366197e-05,
      "loss": 3.5835,
      "step": 2800
    },
    {
      "epoch": 10.035211267605634,
      "grad_norm": 1.990438461303711,
      "learning_rate": 9.93661971830986e-06,
      "loss": 3.5361,
      "step": 2850
    },
    {
      "epoch": 10.211267605633802,
      "grad_norm": 2.4763100147247314,
      "learning_rate": 9.58450704225352e-06,
      "loss": 3.5441,
      "step": 2900
    },
    {
      "epoch": 10.387323943661972,
      "grad_norm": 2.250593423843384,
      "learning_rate": 9.232394366197183e-06,
      "loss": 3.556,
      "step": 2950
    },
    {
      "epoch": 10.56338028169014,
      "grad_norm": 2.24387264251709,
      "learning_rate": 8.880281690140844e-06,
      "loss": 3.5352,
      "step": 3000
    },
    {
      "epoch": 10.73943661971831,
      "grad_norm": 2.6001431941986084,
      "learning_rate": 8.528169014084507e-06,
      "loss": 3.5358,
      "step": 3050
    },
    {
      "epoch": 10.915492957746478,
      "grad_norm": 2.0248682498931885,
      "learning_rate": 8.17605633802817e-06,
      "loss": 3.5594,
      "step": 3100
    },
    {
      "epoch": 11.091549295774648,
      "grad_norm": 2.017338514328003,
      "learning_rate": 7.823943661971831e-06,
      "loss": 3.5536,
      "step": 3150
    },
    {
      "epoch": 11.267605633802816,
      "grad_norm": 2.939500570297241,
      "learning_rate": 7.471830985915493e-06,
      "loss": 3.5319,
      "step": 3200
    },
    {
      "epoch": 11.443661971830986,
      "grad_norm": 2.225299596786499,
      "learning_rate": 7.119718309859155e-06,
      "loss": 3.5141,
      "step": 3250
    },
    {
      "epoch": 11.619718309859154,
      "grad_norm": 2.493093252182007,
      "learning_rate": 6.767605633802817e-06,
      "loss": 3.5092,
      "step": 3300
    },
    {
      "epoch": 11.795774647887324,
      "grad_norm": 1.8095864057540894,
      "learning_rate": 6.4154929577464795e-06,
      "loss": 3.534,
      "step": 3350
    },
    {
      "epoch": 11.971830985915492,
      "grad_norm": 1.8872475624084473,
      "learning_rate": 6.0633802816901415e-06,
      "loss": 3.4751,
      "step": 3400
    },
    {
      "epoch": 12.147887323943662,
      "grad_norm": 1.989912509918213,
      "learning_rate": 5.711267605633803e-06,
      "loss": 3.4824,
      "step": 3450
    },
    {
      "epoch": 12.323943661971832,
      "grad_norm": 2.298337936401367,
      "learning_rate": 5.359154929577465e-06,
      "loss": 3.505,
      "step": 3500
    },
    {
      "epoch": 12.5,
      "grad_norm": 2.152672290802002,
      "learning_rate": 5.007042253521127e-06,
      "loss": 3.5029,
      "step": 3550
    },
    {
      "epoch": 12.676056338028168,
      "grad_norm": 2.3832175731658936,
      "learning_rate": 4.654929577464789e-06,
      "loss": 3.4813,
      "step": 3600
    },
    {
      "epoch": 12.852112676056338,
      "grad_norm": 2.116199493408203,
      "learning_rate": 4.30281690140845e-06,
      "loss": 3.48,
      "step": 3650
    },
    {
      "epoch": 13.028169014084508,
      "grad_norm": 2.37713885307312,
      "learning_rate": 3.950704225352112e-06,
      "loss": 3.4734,
      "step": 3700
    },
    {
      "epoch": 13.204225352112676,
      "grad_norm": 1.7618252038955688,
      "learning_rate": 3.598591549295775e-06,
      "loss": 3.4599,
      "step": 3750
    },
    {
      "epoch": 13.380281690140846,
      "grad_norm": 2.072896718978882,
      "learning_rate": 3.246478873239437e-06,
      "loss": 3.4672,
      "step": 3800
    },
    {
      "epoch": 13.556338028169014,
      "grad_norm": 2.289146661758423,
      "learning_rate": 2.8943661971830987e-06,
      "loss": 3.4774,
      "step": 3850
    },
    {
      "epoch": 13.732394366197184,
      "grad_norm": 2.648369312286377,
      "learning_rate": 2.5422535211267602e-06,
      "loss": 3.4923,
      "step": 3900
    },
    {
      "epoch": 13.908450704225352,
      "grad_norm": 2.2198057174682617,
      "learning_rate": 2.1901408450704226e-06,
      "loss": 3.4778,
      "step": 3950
    },
    {
      "epoch": 14.084507042253522,
      "grad_norm": 2.167293071746826,
      "learning_rate": 1.8380281690140847e-06,
      "loss": 3.4493,
      "step": 4000
    },
    {
      "epoch": 14.26056338028169,
      "grad_norm": 2.650387763977051,
      "learning_rate": 1.4859154929577464e-06,
      "loss": 3.4787,
      "step": 4050
    },
    {
      "epoch": 14.43661971830986,
      "grad_norm": 2.2946887016296387,
      "learning_rate": 1.1338028169014085e-06,
      "loss": 3.482,
      "step": 4100
    },
    {
      "epoch": 14.612676056338028,
      "grad_norm": 1.8699004650115967,
      "learning_rate": 7.816901408450705e-07,
      "loss": 3.4634,
      "step": 4150
    },
    {
      "epoch": 14.788732394366198,
      "grad_norm": 2.5106513500213623,
      "learning_rate": 4.2957746478873244e-07,
      "loss": 3.4355,
      "step": 4200
    },
    {
      "epoch": 14.964788732394366,
      "grad_norm": 1.856981635093689,
      "learning_rate": 7.746478873239437e-08,
      "loss": 3.4543,
      "step": 4250
    }
  ],
  "logging_steps": 50,
  "max_steps": 4260,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 345292585989120.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
